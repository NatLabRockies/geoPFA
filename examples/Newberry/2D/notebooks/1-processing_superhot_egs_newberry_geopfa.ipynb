{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f0a4f98-c874-4c05-9393-2d14af2eaab8",
   "metadata": {},
   "source": [
    "# Data Processing with `geoPFA`: 2D Example from Newberry Volcano, OR\n",
    "This tutorial demonstrates how to use the **data processing workflow** in `geoPFA`.  \n",
    "You’ll learn how to:\n",
    "\n",
    "1. Load a PFA configuration file (which defines criteria, components, and layers)\n",
    "2. Read raw geospatial datasets into the working `pfa` dictionary\n",
    "3. Process and clean these datasets into a consistent grid ready for layer combination and favorability modeling\n",
    "\n",
    "Subsequent notebooks will build upon this by combining layers, applying weights, and producing final favorability models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf1db7c",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c08199-1f29-47e1-86e2-21f2ed64b436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- General imports ---\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "\n",
    "# --- geoPFA core classes ---\n",
    "from geopfa.data_readers import GeospatialDataReaders\n",
    "from geopfa.geopfa2d.processing import Cleaners, Processing  # TODO: change to geoPFA.processing\n",
    "from geopfa.geopfa2d.plotters import GeospatialDataPlotters  # TODO: change to geoPFA.plotters\n",
    "\n",
    "# --- Utilities ---\n",
    "from rex.utilities.utilities import safe_json_load\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b814039f-199f-429c-8020-0d913019d8cd",
   "metadata": {},
   "source": [
    "### Define Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d97b29-c6d7-47ac-bd29-15186d0fa499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the directory where this notebook lives\n",
    "notebook_dir = Path(__file__).resolve().parent if \"__file__\" in locals() else Path.cwd()\n",
    "\n",
    "# Define the main project directory (where 'config/', 'data/', and 'notebooks/' live)\n",
    "project_dir = notebook_dir.parent\n",
    "\n",
    "# Define subdirectories\n",
    "config_dir = project_dir / \"config\"\n",
    "data_dir = project_dir / \"data\"\n",
    "\n",
    "# Confirm structure\n",
    "print(\"Notebook directory:\", notebook_dir)\n",
    "print(\"Config directory:\", config_dir)\n",
    "print(\"Data directory:\", data_dir)\n",
    "\n",
    "# Quick check before proceeding\n",
    "for folder in [config_dir, data_dir]:\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"Expected folder not found: {folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bf1a11",
   "metadata": {},
   "source": [
    "## 2. Configuration and Data Input\n",
    "With the project directories defined, the next step is to **read the configuration file** and **load all raw geospatial data** into memory.\n",
    "\n",
    "Each PFA project is organized according to a hierarchy of criteria → components → layers, where each **layer** represents a dataset.  \n",
    "![dir_structure](figs/pfa_processing.png)\n",
    "\n",
    "The configuration JSON defines:\n",
    "- Which layers belong to which components, and which components belong to which criteria\n",
    "- Layer metadata including names, coordinate reference systems (CRS), and column definitions  \n",
    "- Processing instructions (e.g., interpolation methods, z-measure units)\n",
    "\n",
    "To ensure a smooth workflow:\n",
    "- The structure of the `/data` directory must exactly match those specified in the JSON (see example below). Folder names must match criteria and component names, and file names must match layer names.   \n",
    "- Each dataset must have consistent column names and units with its config entry.\n",
    "- See Configuration Instructions for more \n",
    "\n",
    "![dir_structure](figs/directory_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a819a3",
   "metadata": {},
   "source": [
    "### Read configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179477ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to configuration file\n",
    "pfa_path = config_dir / \"newberry_superhot_config.json\"\n",
    "\n",
    "# Load JSON configuration safely (handles comments and malformed JSON)\n",
    "pfa = safe_json_load(str(pfa_path))\n",
    "\n",
    "# Quick check\n",
    "if not pfa_path.exists():\n",
    "    raise FileNotFoundError(f\"Configuration file not found: {pfa_path}\")\n",
    "\n",
    "pfa = safe_json_load(str(pfa_path))\n",
    "print(f\"Loaded PFA configuration from: {pfa_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b5b9ef",
   "metadata": {},
   "source": [
    "### Load data into the `pfa` dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71fe60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file types to be read (others will be skipped with a warning)\n",
    "file_types = [\".csv\", \".shp\"]\n",
    "\n",
    "# Gather data according to the config structure\n",
    "pfa = GeospatialDataReaders.gather_data(data_dir, pfa, file_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef36117",
   "metadata": {},
   "source": [
    "### Verify that all layers loaded correctly\n",
    "This simple diagnostic ensures that each layer entry in the `pfa` dictionary contains a `\"data\"` key with a valid GeoDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f199b90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_layers = []\n",
    "\n",
    "for criteria, crit_data in pfa.get(\"criteria\", {}).items():\n",
    "    for component, comp_data in crit_data.get(\"components\", {}).items():\n",
    "        for layer, layer_data in comp_data.get(\"layers\", {}).items():\n",
    "            if \"data\" not in layer_data:\n",
    "                missing_data_layers.append({\n",
    "                    \"criteria\": criteria,\n",
    "                    \"component\": component,\n",
    "                    \"layer\": layer,\n",
    "                })\n",
    "\n",
    "if missing_data_layers:\n",
    "    print(\"Layers missing 'data':\")\n",
    "    for item in missing_data_layers:\n",
    "        print(f\" - {item['criteria']} / {item['component']} / {item['layer']}\")\n",
    "else:\n",
    "    print(\"All layers contain 'data'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb06880",
   "metadata": {},
   "source": [
    "## 3. Harmonizing Data\n",
    "\n",
    "Now that all raw layers are loaded into the `pfa` dictionary, we’ll:\n",
    "- Define a target coordinate reference system (CRS)\n",
    "- Constrain grid resolution (`nx` and `ny`)\n",
    "- Import supporting data such as an area outline or a well path to give context\n",
    "- Derive a shared spatial extent for clipping and gridding\n",
    "- Preview the input data layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87362ea",
   "metadata": {},
   "source": [
    "### Define the Target CRS and Import Contextual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84795076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPSG code for UTM Zone 10N (meters)\n",
    "target_crs = 26910\n",
    "# Define grid resolution (number of cells in x and y)\n",
    "nx = 300; ny = 300\n",
    "\n",
    "# Load the project boundary shapefile\n",
    "outline_path = data_dir / \"supporting_data\" / \"national_monument_boundary\" / \"NNVM_bounds.shp\"\n",
    "outline = gpd.read_file(outline_path).to_crs(target_crs)\n",
    "\n",
    "print(f\"Loaded boundary: {outline_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8bc46f",
   "metadata": {},
   "source": [
    "### Unify CRS and Define Extent\n",
    "CRS is defined manually using the variable `target_crs` defined above. \n",
    "Extent is defined using the defined `extent_layer` above, where the project extent is set to that of the specified `extent_layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260fbe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproject all layers to the target CRS\n",
    "pfa = Cleaners.set_crs(pfa, target_crs=target_crs)\n",
    "\n",
    "# Choose one representative layer to define the spatial extent\n",
    "extent_layer = (\n",
    "    pfa[\"criteria\"][\"geologic\"][\"components\"][\"heat\"][\"layers\"][\"mt_resistivity_joint_inv\"][\"data\"]\n",
    ")\n",
    "extent = Cleaners.get_extent(extent_layer)\n",
    "\n",
    "# Validate the extent\n",
    "print(f\"Extent: {extent}\")\n",
    "if extent[0] >= extent[2] or extent[1] >= extent[3]:\n",
    "    raise ValueError(\"Invalid extent: min values must be less than max values.\")\n",
    "\n",
    "# Define the active criterion (geologic only in this example)\n",
    "criteria = \"geologic\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9741c50d-4ae8-4c96-b510-4481636b8938",
   "metadata": {},
   "source": [
    "## 4. Convert 3D Layers to 2D\n",
    "\n",
    "Many datasets in a PFA project are generated or modeled in 3D, but 2D PFAs\n",
    "often only require a surface representation.  \n",
    "This step uses `Processing.convert_3d_to_2d()` to **collapse each 3D layer along the Z-dimension**\n",
    "into a 2D `GeoDataFrame`. \n",
    "\n",
    "Instead of taking a slice at a given depth, this function sums over Z and each X,Y location, provinding insight into thickness of features. If you'd prefer to use a slice at a given depth, use that 2D slice as your input data layer rather than inputting the 3D model.\n",
    "\n",
    "Layers are only flattened if:\n",
    "- They are marked as `\"is_3d\": \"yes\"` in the configuration file  \n",
    "- They are **not** explicitly excluded (`\"needs_flattening\": \"no\"`)\n",
    "\n",
    "To save time, each unique layer is flattened **only once** and then reused for any other component that references it.\n",
    "This is especially useful for large shared datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c4fe39-dcac-4f8a-84df-b56fd590dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache to store flattened layers and reuse them\n",
    "flattened_layers = {}\n",
    "\n",
    "for component, comp_data in pfa[\"criteria\"][criteria][\"components\"].items():\n",
    "    for layer, layer_config in comp_data[\"layers\"].items():\n",
    "        is_3d = layer_config.get(\"is_3d\", None)\n",
    "        needs_flattening = layer_config.get(\"needs_flattening\", \"yes\")\n",
    "\n",
    "        # Skip layers not flagged as 3D or marked \"no\" for flattening\n",
    "        if not is_3d or str(needs_flattening).lower() == \"no\":\n",
    "            continue\n",
    "\n",
    "        # Use the layer name as the unique cache key\n",
    "        key = layer\n",
    "\n",
    "        if key in flattened_layers:\n",
    "            print(f\"Reusing flattened layer: {layer} for component: {component}\")\n",
    "            # Copy cached flattened layer into current component/layer\n",
    "            pfa[\"criteria\"][criteria][\"components\"][component][\"layers\"][layer] = (\n",
    "                flattened_layers[key].copy()\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        print(f\"Converting 3D layer: {layer} → 2D\")\n",
    "\n",
    "        # Perform flattening and store in PFA\n",
    "        pfa = Processing.convert_3d_to_2d(pfa, criteria, component=component, layer=layer)\n",
    "\n",
    "        # Cache the flattened result for reuse\n",
    "        flattened_layers[key] = (\n",
    "            pfa[\"criteria\"][criteria][\"components\"][component][\"layers\"][layer].copy()\n",
    "        )\n",
    "\n",
    "print(\"3D → 2D flattening complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c673990-8927-4666-8f19-507be43e571f",
   "metadata": {},
   "source": [
    "### Visualize Cleaned Raw Data Layers\n",
    "`geoPFA` has built-in plotting tools to help visualize data. Some examples are shown below using `GeospatialDataPlotters.geo_plot_3d`.\n",
    "#### Plot a Single Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf700d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "component = \"heat\"\n",
    "layer = \"mt_resistivity_joint_inv\"\n",
    "\n",
    "gdf = pfa[\"criteria\"][criteria][\"components\"][component][\"layers\"][layer][\"data\"]\n",
    "col = pfa[\"criteria\"][criteria][\"components\"][component][\"layers\"][layer][\"data_col\"]\n",
    "units = pfa[\"criteria\"][criteria][\"components\"][component][\"layers\"][layer][\"units\"]\n",
    "title = f\"{layer}: raw data\"\n",
    "\n",
    "GeospatialDataPlotters.geo_plot(gdf, col, units, title, markersize=2, figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ee7488",
   "metadata": {},
   "source": [
    "#### Plot All Layers (Optional)\n",
    "\n",
    "Loop through the full hierarchy and visualize every dataset.\n",
    "This is useful for verifying that CRS, units, and extents align, but can take time for large inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e212d-e2c4-4dbd-95f5-6b3dd5ef2d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for criteria, crit_data in pfa[\"criteria\"].items():\n",
    "    print(criteria)\n",
    "    plotted_layers = []\n",
    "    for component, comp_data in crit_data[\"components\"].items():\n",
    "        print(f\"\\t{component}\")\n",
    "        for layer, layer_data in comp_data[\"layers\"].items():\n",
    "            if layer in plotted_layers:\n",
    "                print(f\"\\t\\t{layer} plotted already\")\n",
    "                continue\n",
    "\n",
    "            gdf = layer_data[\"data\"]\n",
    "            col = layer_data.get(\"data_col\", None)\n",
    "            if layer == \"faults_newberry\":\n",
    "                col = \"None\"  # faults are geometries only\n",
    "\n",
    "            units = layer_data[\"units\"]\n",
    "            title = f\"{layer}: raw data\"\n",
    "\n",
    "            GeospatialDataPlotters.geo_plot(gdf, col, units, title, markersize=2, figsize=(5, 5))\n",
    "            plotted_layers.append(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675c4e58-c957-47eb-8a67-85a302b692a9",
   "metadata": {},
   "source": [
    "## 5. Data Processing and Preparation\n",
    "This step transforms raw geospatial inputs into standardized layers on a shared 2D grid for comparison and combination.\n",
    "\n",
    "Examples:\n",
    "- *Permeability indicators*: distance to faults, earthquake density  \n",
    "- *Heat indicators*: resistivity, temperature, low density  \n",
    "- *Insulation indicators*: shallow resistivity, seismic velocity  \n",
    "\n",
    "Processing ensures all layers:\n",
    "- Are free from outliers  \n",
    "- Share the same CRS and spatial extent  \n",
    "- Are mapped to the same `(nx, ny)` grid  \n",
    "- Represent continuous 2D evidence produced from the data layer (e.g. distance from faults as opposed to fault location)\n",
    "---\n",
    "\n",
    "### How It Works\n",
    "\n",
    "All 2D `geoPFA` processing functions follow the same pattern:\n",
    "\n",
    "1. Take the **raw input** stored in  \n",
    "   `pfa[\"criteria\"][criteria][\"components\"][component][\"layers\"][layer][\"data\"]`\n",
    "2. Process it (e.g., interpolation, point density, distance-to-feature)  \n",
    "   onto the common 2D grid\n",
    "3. Write the result to  \n",
    "   `pfa[\"criteria\"][criteria][\"components\"][component][\"layers\"][layer][\"model\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3231617b-ee7b-4f1a-85e2-00298c80e85d",
   "metadata": {},
   "source": [
    "### Filter Outliers\n",
    "Some datasets contain spurious extreme values.\n",
    "Use Cleaners.filter_geodataframe() to remove these outliers based on a quantile threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e7db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the upper 10% of values in selected datasets\n",
    "target_layers = []\n",
    "\n",
    "for layer in target_layers:\n",
    "    for component in [\"heat\", \"reservoir\"]:\n",
    "        gdf = pfa[\"criteria\"][criteria][\"components\"][\"heat\"][\"layers\"][layer][\"data\"]\n",
    "        col = pfa[\"criteria\"][criteria][\"components\"][\"heat\"][\"layers\"][layer][\"data_col\"]\n",
    "\n",
    "        filtered = Cleaners.filter_geodataframe(gdf, col, quantile=0.9)\n",
    "        pfa[\"criteria\"][criteria][\"components\"][component][\"layers\"][layer][\"data\"] = filtered\n",
    "\n",
    "print(\"✅ Outlier filtering complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ec2e05-b1c0-4ebf-9378-d6d8a10f4caf",
   "metadata": {},
   "source": [
    "### Interpolation\n",
    "For most datasets, data can be processed and prepared for layer combination and favorability estimation by simply interpolating data to the target grid. Here we do that only once for each input layer that has `\"processing_method\": \"interpolate\"` specified in the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03426390",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'linear'\n",
    "\n",
    "interpolated_layers = {}\n",
    "\n",
    "for criteria, crit_data in pfa[\"criteria\"].items():\n",
    "    print(criteria)\n",
    "    for component, comp_data in crit_data[\"components\"].items():\n",
    "        print(f\"\\t{component}\")\n",
    "        for layer, layer_config in comp_data[\"layers\"].items():\n",
    "            if layer_config.get(\"processing_method\") == \"interpolate\":\n",
    "                if layer in interpolated_layers:\n",
    "                    print(f\"\\t\\tReusing cached interpolation for {layer}\")\n",
    "                    pfa[\"criteria\"][criteria][\"components\"][component][\"layers\"][layer]['model'] = (\n",
    "                        interpolated_layers[layer]['model'].copy()\n",
    "                    )\n",
    "                    pfa[\"criteria\"][criteria][\"components\"][component][\"layers\"][layer]['model_data_col'] = (\n",
    "                        interpolated_layers[layer]['model_data_col']\n",
    "                    )\n",
    "                    pfa[\"criteria\"][criteria][\"components\"][component][\"layers\"][layer]['model_units'] = (\n",
    "                        interpolated_layers[layer]['model_units']\n",
    "                    )\n",
    "                else:\n",
    "                    print(f\"\\t\\tInterpolating layer: {layer}\")\n",
    "                    pfa = Processing.interpolate_points(\n",
    "                        pfa,\n",
    "                        criteria=criteria,\n",
    "                        component=component,\n",
    "                        layer=layer,\n",
    "                        interp_method=method,\n",
    "                        nx=nx,\n",
    "                        ny=ny,\n",
    "                        extent=extent,\n",
    "                    )\n",
    "                    interpolated_layers[layer] = (\n",
    "                        pfa[\"criteria\"][criteria][\"components\"][component][\"layers\"][layer].copy()\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33e0e92-b816-4b2e-981d-a37e5fb64045",
   "metadata": {},
   "source": [
    "### Point Density to be Used as a Favorability Proxy\n",
    "\n",
    "For some layers, the spatial density of points (e.g., earthquake occurrences) is an indicator of permeability or fracturing. `geoPFA` has two options to do this in 2D, including:\n",
    " - `Processing.point_density`: Computes 2D density projected onto user-defined grids (ofter coarser than the project grid), ,and then project those onto the project grid to allow PFA computations that rely on harmonized grids.\n",
    " - `Processing.weighted_distance_from_points`: Compute a weighted influence field from points over the 2D project grid, using distance to control \"influence,\" optionallly weighted by an attribute of the user's choice (e.g., earthquake magnitude).\n",
    "\n",
    "In this example, we use `Processing.weighted_distance_from_points` because it produces smoother results and alllows us to incorporate earthquake magnitude.\n",
    "> *Tuning Tip:* Larger `alpha` values spread each equarthquake’s influence farther; smaller values make it more localized. In other words, a larger value for `alpha` might be preferably when wworking with relatively sparse earthquakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5d6b4e-a50e-4bfe-8376-e547ee0f2737",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = 'geologic'\n",
    "layer = 'earthquakes'\n",
    "# Filter extreme densities\n",
    "for comp in [\"heat\", \"reservoir\", \"insulation\"]:\n",
    "    pfa = Processing.weighted_distance_from_points(pfa, criteria=criteria, component=comp, \n",
    "                               layer=layer, extent=extent, nx=nx, ny=ny, alpha=1000)\n",
    "    gdf = pfa[\"criteria\"][criteria][\"components\"][comp][\"layers\"][layer][\"model\"].copy()\n",
    "    col = pfa[\"criteria\"][criteria][\"components\"][comp][\"layers\"][layer][\"model_data_col\"]\n",
    "    filtered = Cleaners.filter_geodataframe(gdf, col, quantile=0.9)\n",
    "    pfa[\"criteria\"][criteria][\"components\"][comp][\"layers\"][layer][\"model\"] = filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cce4974-b199-44b2-b088-c586cf89742f",
   "metadata": {},
   "source": [
    "### Process Faults\n",
    "\n",
    "Faults often act as preferential pathways for fluid flow, making them key indicators for the *reservoir* component.  \n",
    "Here, we use the built-in `process_faults()` function to calculate a **fault-based favorability score** that increases with proximity to faults and (optionally) their intersections.\n",
    "\n",
    "The function performs:\n",
    "1. **Distance calculation** – measures distance from each grid point to nearby faults.  \n",
    "2. **Intersection detection** (optional) – identifies where faults intersect and computes distance to those points.  \n",
    "3. **Exponential decay weighting** – applies user-defined decay constants (`alpha_fault`, `alpha_intersection`) to represent how influence decreases with distance.  \n",
    "4. **Weighted combination** – combines both effects using `weight_fault` and `weight_intersection`.\n",
    "\n",
    "> *Tuning Tip:* Larger `alpha` values spread the fault’s influence farther; smaller values make it more localized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd077c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "component = \"reservoir\"\n",
    "layer = \"lineaments\"\n",
    "\n",
    "# --- Fault processing for reservoir component ---\n",
    "pfa = Processing.process_faults(\n",
    "    pfa,\n",
    "    criteria=criteria,\n",
    "    component=component,\n",
    "    layer=layer,\n",
    "    extent=extent,\n",
    "    nx=nx,\n",
    "    ny=ny,\n",
    "    alpha_fault=5500.0,        # decay distance for faults\n",
    "    alpha_intersection=3500.0, # decay distance for intersections\n",
    "    weight_fault=0.7,          # relative weighting of faults vs intersections\n",
    "    weight_intersection=0.3,\n",
    "    use_intersections=True,    # include intersection effects\n",
    ")\n",
    "\n",
    "#  Visualize the results\n",
    "#  Note the plotted extent differs from the raw data when comparing\n",
    "\n",
    "# Note we now grab the processed data from its \"model\" locations\n",
    "gdf = pfa[\"criteria\"][criteria][\"components\"][component][\"layers\"][layer][\"model\"]\n",
    "col = pfa[\"criteria\"][criteria][\"components\"][component][\"layers\"][layer][\"model_data_col\"]\n",
    "units = pfa[\"criteria\"][criteria][\"components\"][component][\"layers\"][layer][\"model_units\"]\n",
    "title = f\"{layer}: raw data\"\n",
    "\n",
    "GeospatialDataPlotters.geo_plot(gdf, col, units, title, markersize=2, figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564227d3-eb9e-4969-9d40-2e7afca71506",
   "metadata": {},
   "source": [
    "### Extrapolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90ac925-0e9f-4230-b797-95453acc49ea",
   "metadata": {},
   "source": [
    "`geoPFA` can perform extrapolation over each layer to ensure every layer fills the complete extent. `geoPFA` uses a sparse Gaussian process regression (GPR) model to backfill missing data. Extrapolation works best when the extent of the missing data is relatively small compared to the known data. For large areas, `geoPFA` reverst to a linear trend across the training site to produce essentially the average spatial proess of the known data.\n",
    "\n",
    "For large datasets, extrapolation can take substanial processing time to complete. Users can improve model fitting time by reducing the `training_size`. Datasets with relatively complete information will benefit greatly from reduced training sizes and should see little reduction in performance. An optional `verbose` flag will enable model assessment and reporting along with model diagnostic and final prediction plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47a16f2-a185-4364-a9c7-8e8822ee3d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopfa.processing import Processing\n",
    "\n",
    "extrapolated_layers = {}\n",
    "\n",
    "for criteria, crit_data in pfa[\"criteria\"].items():\n",
    "    print(criteria)\n",
    "    for component, comp_data in crit_data[\"components\"].items():\n",
    "        print(f\"\\t{component}\")\n",
    "        for layer, layer_config in comp_data[\"layers\"].items():\n",
    "            if (\n",
    "                layer_config[\"model\"][layer_config[\"model_data_col\"]]\n",
    "                .isna()\n",
    "                .sum(axis=0)\n",
    "            ):\n",
    "                if layer in extrapolated_layers:\n",
    "                    print(f\"\\t\\tReusing cached extrapolation for {layer}\")\n",
    "                    pfa[\"criteria\"][criteria][\"components\"][component][\n",
    "                        \"layers\"\n",
    "                    ][layer] = extrapolated_layers[layer].copy()\n",
    "                else:\n",
    "                    print(f\"\\t\\tExtrapolating layer: {layer}\")\n",
    "\n",
    "                    dataset = \"model\"\n",
    "                    data_col = \"value_interpolated\"\n",
    "\n",
    "                    pfa = Processing.extrapolate_2d(\n",
    "                        pfa,\n",
    "                        criteria=criteria,\n",
    "                        component=component,\n",
    "                        layer=layer,\n",
    "                        dataset=dataset,\n",
    "                        data_col=data_col,\n",
    "                        training_size=0.8,\n",
    "                        verbose=False,\n",
    "                    )\n",
    "\n",
    "                    extrapolated_layers[layer] = pfa[\"criteria\"][criteria][\n",
    "                        \"components\"\n",
    "                    ][component][\"layers\"][layer].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f162e010-d064-406e-b25a-440a8b0b40a2",
   "metadata": {},
   "source": [
    "### Plot All Processed Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77278a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "for criteria, crit_data in pfa[\"criteria\"].items():\n",
    "    print(criteria)\n",
    "    plotted_layers = []\n",
    "\n",
    "    for component, comp_data in crit_data[\"components\"].items():\n",
    "        print(f\"\\t{component}\")\n",
    "\n",
    "        for layer, layer_data in comp_data[\"layers\"].items():\n",
    "            if layer in plotted_layers:\n",
    "                continue\n",
    "\n",
    "            # Skip if no processed model exists\n",
    "            if \"model\" not in layer_data or layer_data[\"model\"] is None:\n",
    "                print(f\"\\t\\t⚠️ Skipping {layer}: no processed 'model' found.\")\n",
    "                continue\n",
    "\n",
    "            gdf = layer_data[\"model\"]\n",
    "            col = layer_data.get(\"model_data_col\")\n",
    "            units = layer_data.get(\"model_units\", \"None\")\n",
    "            title = f\"{layer}: processed model\"\n",
    "\n",
    "            # Plot in 3D\n",
    "            GeospatialDataPlotters.geo_plot(\n",
    "                gdf, col, units, title, area_outline=outline, extent=extent, markersize=0.75, figsize=(5, 5)\n",
    "            )\n",
    "\n",
    "            plotted_layers.append(layer)\n",
    "\n",
    "print(\"\\n✅ Finished plotting all available processed layers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ddae35-2394-44c0-aad3-41801a19acd9",
   "metadata": {},
   "source": [
    "## Save Processed Layers\n",
    "It’s good practice to save processed data to disk before proceeding so processing only has to be completed once. This allows the user to make minor tweaks to the PFA, such as adjusting weights, without reprocessing all data layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cab4b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for criteria, crit_data in pfa[\"criteria\"].items():\n",
    "    print(criteria)\n",
    "    for component, comp_data in crit_data[\"components\"].items():\n",
    "        print(f\"\\t{component}\")\n",
    "        for layer, layer_data in comp_data[\"layers\"].items():\n",
    "            gdf = layer_data[\"model\"]\n",
    "            col = layer_data[\"model_data_col\"]\n",
    "            units = layer_data[\"model_units\"]\n",
    "\n",
    "            out_fp = data_dir / criteria / component / f\"{layer}_processed.csv\"\n",
    "            gdf.to_csv(out_fp, index=False)\n",
    "            print(f\"\\t\\tSaved: {out_fp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dc4a0a",
   "metadata": {},
   "source": [
    "### Save a “Clean” PFA Config (no GeoDataFrames)\n",
    "Similarly, saving a \"clean\" processed PFA config allows the user to avoid repeating the processing step when making minor adjustments to the PFA. This processed config serves as the config file for the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fa30e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_geodataframes(data: dict) -> dict:\n",
    "    \"\"\"Recursively remove GeoDataFrame objects before saving.\"\"\"\n",
    "    clean = {}\n",
    "    for key, val in data.items():\n",
    "        if isinstance(val, gpd.GeoDataFrame):\n",
    "            continue\n",
    "        elif isinstance(val, dict):\n",
    "            clean[key] = drop_geodataframes(val)\n",
    "        else:\n",
    "            clean[key] = val\n",
    "    return clean\n",
    "\n",
    "pfa_nodf = drop_geodataframes(pfa)\n",
    "out_json = config_dir / \"newberry_superhot_processed_config.json\"\n",
    "with open(out_json, \"w\") as f:\n",
    " json.dump(pfa_nodf, f, indent=4)\n",
    "print(f\"Processed PFA configuration saved to: {out_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c868b9d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps: Layer Combination and Favorability Modeling\n",
    "\n",
    "This concludes **Notebook 1 – Data Processing with `geoPFA`**.\n",
    "\n",
    "In this notebook, we:\n",
    "- Loaded, cleaned, and standardized all input datasets  \n",
    "- Processed each layer (filtering, interpolation, density, distance, transformations)  \n",
    "- Saved the resulting **processed layers** to disk\n",
    "\n",
    "You’re now ready to combine these layers into component and criteria favorability models.\n",
    "\n",
    "If you skip running this notebook, don’t worry — the **supplementary example data** includes all intermediate processed files.  \n",
    "You can start directly with **Notebook 2 – Layer Combination and Favorability Modeling**, which loads those preprocessed layers automatically.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94596fbb-d0d7-4d0b-a64d-d82f86671bac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
